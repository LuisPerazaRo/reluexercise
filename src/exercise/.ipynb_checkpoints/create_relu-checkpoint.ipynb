{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "848723d0-e468-4e39-9fab-ea6bd978c3ce",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# The ReLU activation function\n",
    "### Building your first neural network in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da7892b-544b-440b-8d0b-c2ec6e0f6e75",
   "metadata": {},
   "source": [
    "#### What is ReLU?\n",
    "\n",
    "ReLU stands for Rectified Linear Unit. It is one of the most widely used activation functions in deep learning, especially in convolutional neural networks (CNNs) and fully connected layers.\n",
    "\n",
    "Mathematically, ReLU is defined as:\n",
    "\n",
    "$$\n",
    "\\text{ReLU}(x) = \\max(0, x)\n",
    "$$\n",
    "\n",
    "This means:\n",
    "\n",
    "$$\n",
    "\\text{If } ( x > 0 ), \\text{ReLU returns } x.\n",
    "$$\n",
    "$$\n",
    "\\text{If } ( x \\leq 0 ), \\text{ReLU returns } 0.\n",
    "$$\n",
    "\n",
    "In simple terms:\n",
    "ReLU \"rectifies\" negative values to zero and keeps positive values unchanged.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fd1448-5e32-49e2-b188-bb4fcce49aee",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div align=\"center\">\n",
    "  <img src=\"../assets/ReLU.png\" alt=\"ReLU function\" width=\"50%\"/>\n",
    "  <p style=\"text-align: center;\"><em>Figure 1: ReLU activation function</em></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c7c9d8-a272-4126-a0c3-6b6039a958c2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Why Do We Need ReLU?\n",
    "\n",
    "Before ReLU became popular, activation functions like sigmoid and tanh were commonly used, see Figure 2. Their purpose is to introduce a non-linearity into the neural network and they were easy to differentiate when the mathematics of neural networks was first developed plus other desired properties like being monotically increasing. However, these functions introduced two major problems:\n",
    "\n",
    "**Vanishing Gradient Problem**\n",
    "\n",
    "In deep neural networks, gradients from sigmoid or tanh can become very small during backpropagation, especially when activations are in their saturated regions where the derivative is small. This makes learning slow or even stops it completely.\n",
    "\n",
    "**Expensive Computation**\n",
    "\n",
    "Sigmoid and tanh involve exponential functions, making them computationally more expensive compared to simple operations like max(0, x) in ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4fb1de-96a6-4269-84ef-e06b677401e9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div align=\"center\"> \n",
    "  <img src=\"../assets/Activation_logistic.svg\" alt=\"Logistic activation function\" width=\"40%\" style=\"background-color: white;\"/>\n",
    "  <p style=\"text-align: center;\"><em>Figure 2a: Sigmoid activation function</em></p>\n",
    "</div>\n",
    "\n",
    "<div align=\"center\"> \n",
    "  <img src=\"../assets/Activation_tanh.svg\" alt=\"Hyperbolic tangent activation function\" width=\"40%\" style=\"background-color: white;\"/>\n",
    "  <p style=\"text-align: center;\"><em>Figure 2b: Hyperbolic tangent (tanh) activation function</em></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adefcb2-5b64-4ba9-8c32-c7c43ddfa6e1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### How ReLU Solves These Problems?\n",
    "\n",
    "**No Saturation for Positive Inputs**\n",
    "\n",
    "ReLU does not saturate in the positive region, which helps maintain stronger gradients during training while still introducing a non-linearity into the network computations.\n",
    "\n",
    "**Computational Efficiency**\n",
    "\n",
    "ReLU only requires a simple thresholding at zero, allowing only values large than zero and zeroing out negative value. This is fast to compute with a clamp operation in [Pytorch](https://docs.pytorch.org/docs/stable/generated/torch.clamp.html). \n",
    "\n",
    "**Encourages Sparse Activations**\n",
    "\n",
    "Since negative inputs become zero, many neurons stay inactive. This sparsity can improve generalization and efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617a3b3d-15f4-428e-8a63-455618db3ebd",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94eb7b4a-a1ab-4c44-81ea-b426c22f6f06",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Exercise: Implementing the ReLU function with Pytorch\n",
    "\n",
    "Read the the Pytorch documentation for the [ReLU module](https://docs.pytorch.org/docs/stable/generated/torch.nn.ReLU.html) to familiarize with its definition. In this excercise we will aim to define our own ReLU module class with a similar behaviour than the Pytorch implementation.\n",
    "\n",
    "Notice in the documentation the ReLU declaration only accepts one argument, `inplace=` which is set to `False` by default. When `inplace=True`, the ReLU modifies the tensor without creating a new tensor, this is only used when saving memory is necessary for the network architecture.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd73f95-3975-4ab8-8ded-9be2f9e84ad5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We will implement the ReLU operation with the Pytorch function [**clamp** (Link)](https://docs.pytorch.org/docs/stable/generated/torch.clamp.html) available as an attribute method of the Tensor object. Clamp exists in two forms: The standard clamp that returns a new object (e.g. `newtensor=oldtensor.clamp`) and its inplace version `.clamp_`. See the documentation https://docs.pytorch.org/docs/stable/generated/torch.Tensor.clamp_.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6d8805-0092-40cc-98e9-fe40bb76d4b0",
   "metadata": {},
   "source": [
    "#### Complete the following code that declared a ReLU module class MyReLU:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4651c1-6756-47c6-ac10-54639a9dbcc7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "`Important`: Do not delete the cell below neither change the name of MyReLU class, the notebook evaluator will search for the cell tag and the class name to assess your work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7855c8-ae57-419e-a266-10be6103c70c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "reluclass"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyReLU(nn.Module):\n",
    "    def __init__(self, inplace=False):\n",
    "        super(MyReLU, self).__init__()\n",
    "        #** Your code goes here **\n",
    "\n",
    "    def forward(self, tinput: torch.Tensor):\n",
    "        if self.inplace:\n",
    "            #** Your code goes here** : Hint: tinput is a Tensor with functional attributes and forward() returns a tensor\n",
    "            pass\n",
    "        else:\n",
    "            ##** Your code goes here** : Hint: tinput is a Tensor with functional attributes and forward() returns a tensor\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3be86b1-44df-40be-a8ce-4ba6f92d7b8d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Test your MyReLU class to show it behaves as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb531b0-299f-43b9-b29c-f7e50e4963b5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_tensor = torch.arange(-10,10,1)\n",
    "print(\"Original test tensor: \\n\", test_tensor)\n",
    "relu = MyReLU()\n",
    "print(\"Test tensor after ReLU operation: \\n\", relu(test_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02e6819-4e25-4733-bc84-ceba99f00b51",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "All negative values in the tensor vector must be zero. Now experiment with random tensors as further check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fe58a4-8b8f-48f1-b4de-4077b269383b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=5\n",
    "test_tensor = torch.randn((n,n))\n",
    "print(\"Original test tensor: \\n\", test_tensor)\n",
    "\n",
    "#Feel free to experiment by yourself with your own code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa48b07-d2df-4b95-8f2e-eadd0e8c0158",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Inplace operation of MyReLU\n",
    "\n",
    "If you coded your MyReLU with `inplace` capability correctly, the code below should displayed two PASSED messages for each of the inplace and not-inplace tests. \n",
    "\n",
    "Feel free to copy-paste the code below into an LLM for an explanation about what it does and checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbdaa42-8ddd-4485-9161-74fe304c7efc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Original tensor\n",
    "x = torch.tensor([1.0, -2.0, 3.0, -4.0, 5.0])\n",
    "\n",
    "print(\"Original Tensor:\", x)\n",
    "print(\"id(x):\", id(x))\n",
    "print(\"data_ptr(x):\", x.data_ptr())\n",
    "print(\"-\" * 40)\n",
    "\n",
    "#Re-instantiating a MyReLU object\n",
    "relu = MyReLU()\n",
    "relu_inplace = MyReLU(inplace=True)\n",
    "\n",
    "# Non-in-place ReLU\n",
    "y = relu(x)\n",
    "print(\"After torch.relu(x) (NOT inplace):\")\n",
    "print(\"Tensor y:\", y)\n",
    "print(\"id(y):\", id(y))\n",
    "print(\"data_ptr(y):\", y.data_ptr())\n",
    "if id(x) != id(y) and x.data_ptr() != y.data_ptr():\n",
    "    print(\"The non-implace instance return DIFFERENT tensors:\", \"PASSED\")\n",
    "else:\n",
    "    print(\"In place operation FAILED.\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# In-place ReLU\n",
    "y = relu_inplace(x)\n",
    "print(\"After x.relu_() (INPLACE):\")\n",
    "print(\"Tensor x:\", y)\n",
    "print(\"id(x):\", id(y))\n",
    "print(\"data_ptr(x):\", y.data_ptr())\n",
    "if id(x) == id(y) and x.data_ptr() == y.data_ptr():\n",
    "    print(\"The non-implace instance return SAME tensors:\", \"PASSED\")\n",
    "else:\n",
    "    print(\"In place operation FAILED.\")\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7638ab7-4638-4086-88b4-2daddc83cb36",
   "metadata": {},
   "source": [
    "**If you got two \"PASSED\" messages, you are ready to evaluate your work in this notebook. Proceed to run the Evaluation cell**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817911c2-cc52-4e23-9339-e56b4bdaf313",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Run the following cell to evaluate your work in this notebook. The code will run similar tests to those you ran above to thoroughly assess your work. You must get FOUR passes in this evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac55b4f0-5fe8-4bbe-a1bc-5c3eed80c9d2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pytest -v ../test_exercise/test_relu.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1a2a8e-61c0-4161-b10e-f928000c09f2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d50c7e3-1794-456b-beca-9f47f3d79f23",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Limitations of ReLU\n",
    "\n",
    "ReLU is not perfect. A well-known issue is the \"dying ReLU\" problem, where neurons can get stuck outputting zero for all inputs if their weights lead to negative pre-activations. Variants like Leaky ReLU and Parametric ReLU (PReLU) were introduced to address this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f7f74f-3f16-4d86-a764-b7acdf1c8894",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Quick Summary\n",
    "\n",
    "Definition: \n",
    "$$\n",
    "\\text{ReLU}(x) =\n",
    "\\begin{cases}\n",
    "x & \\text{if } x > 0 \\\\\n",
    "0 & \\text{if } x \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "Benefits: Reduces vanishing gradients, faster computation, sparse activations\n",
    "\n",
    "Drawback: Dying ReLU (inactive neurons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61472fdf-94ad-48b9-b07e-355ab7386e29",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Solution\n",
    "\n",
    "If you wish, you can have look at hidden the solution below. We recommend to first to attempt this exercise on your own first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334b8f83-fd39-4b21-92cc-e4cf1b4f8fc5",
   "metadata": {},
   "source": [
    "```\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyReLU(nn.Module):\n",
    "    def __init__(self, inplace=False):\n",
    "        super(MyReLU, self).__init__()\n",
    "        self.inplace = inplace\n",
    "    \n",
    "    def forward(self, tinput: torch.Tensor):\n",
    "        if self.inplace:\n",
    "            return tinput.clamp_(min=0)\n",
    "            pass\n",
    "        else:\n",
    "            return tinput.clamp(min=0)\n",
    "            pass\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
